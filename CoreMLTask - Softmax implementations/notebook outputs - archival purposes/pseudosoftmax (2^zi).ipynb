{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hVskP0Dk0Tm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Declaring libraries and downloading dataset\n"
      ],
      "metadata": {
        "id": "MG9S1fwbk3ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Gumbel\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "TuZK2Bfbk6HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CIFAR100(root = 'data/', download = True, transform = ToTensor())\n",
        "test_dataset = CIFAR100(root = 'data/', train = False, transform = ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNzPPnD0lCbq",
        "outputId": "edde99ae-ca41-44a0-cedd-7ca16508e106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:01<00:00, 98498873.92it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-100-python.tar.gz to data/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))\n",
        "print(len(test_dataset))\n",
        "\n",
        "classes = dataset.classes \n",
        "print('Number of Classes:', len(classes))\n",
        "print('Class Names :\\n', classes)\n",
        "\n",
        "#shape of the image tensor\n",
        "img, label = dataset[31]\n",
        "img_shape = img.shape\n",
        "img_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kQcGAKOlG4d",
        "outputId": "381695a7-a84d-4271-dc58-5133419b1876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "10000\n",
            "Number of Classes: 100\n",
            "Class Names :\n",
            " ['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"1 - Max & Min Pixel Values:\", \"Max:\", torch.max(img), \"Min:\", torch.min(img))\n",
        "print(\"2 - Pixel Values for all Channels:\\n\", img[:, 20:25, 20:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y7L7ZnrlM9w",
        "outputId": "6678d4da-2a41-46b2-f677-48374dfcc06d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 - Max & Min Pixel Values: Max: tensor(1.) Min: tensor(0.1020)\n",
            "2 - Pixel Values for all Channels:\n",
            " tensor([[[0.5569, 0.6039, 0.6510, 0.6039, 0.8235],\n",
            "         [0.3961, 0.5922, 0.7529, 0.6824, 0.8314],\n",
            "         [0.2941, 0.5294, 0.6275, 0.6078, 0.6784],\n",
            "         [0.3255, 0.4471, 0.4353, 0.4235, 0.4275],\n",
            "         [0.4667, 0.4667, 0.5020, 0.5725, 0.4706]],\n",
            "\n",
            "        [[0.4353, 0.4745, 0.5020, 0.4549, 0.6510],\n",
            "         [0.3020, 0.4706, 0.5882, 0.5098, 0.6706],\n",
            "         [0.2471, 0.4196, 0.4745, 0.4431, 0.5333],\n",
            "         [0.2667, 0.3529, 0.3098, 0.2941, 0.3098],\n",
            "         [0.3922, 0.4039, 0.4196, 0.4902, 0.3922]],\n",
            "\n",
            "        [[0.4392, 0.4824, 0.5137, 0.4627, 0.6667],\n",
            "         [0.3020, 0.4745, 0.6000, 0.5216, 0.6745],\n",
            "         [0.2431, 0.4353, 0.4980, 0.4627, 0.5373],\n",
            "         [0.2627, 0.3569, 0.3176, 0.2980, 0.3059],\n",
            "         [0.3490, 0.3490, 0.3647, 0.4314, 0.3333]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset"
      ],
      "metadata": {
        "id": "aB93rwDslR-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(2718) ##settling a manual seed to get similar results everytime\n",
        "\n",
        "val_size = 10000\n",
        "train_size = len(dataset) - val_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset,[train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L7WPilYlUrT",
        "outputId": "3e78ac19-a8a7-421b-d5a0-1bbf172b7ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128"
      ],
      "metadata": {
        "id": "Awkhy5FMlfG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defininf Loaders for training process\n",
        "train_loader = DataLoader(train_ds, batch_size, shuffle = True, num_workers=4, pin_memory =True)\n",
        "val_loader = DataLoader(val_ds, batch_size, num_workers=4, pin_memory =True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, num_workers=4, pin_memory =True)"
      ],
      "metadata": {
        "id": "b_RTR5g1lm4z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfa9a55-5abf-43c7-fd09-32b83b70f50e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()\n",
        "\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_default_device()\n",
        "device\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "\n",
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ],
      "metadata": {
        "id": "7ZeufDWHltRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "idk\n"
      ],
      "metadata": {
        "id": "ffs_I-_KmP0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
      ],
      "metadata": {
        "id": "zqopX4ULmR1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using ResNet 9 structure to train the model"
      ],
      "metadata": {
        "id": "6wYk3CwbmfKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_lowest_values(tensor):\n",
        "    num_elements = tensor.numel()  # Total number of elements in the tensor\n",
        "    num_values_to_drop = int(num_elements * 0.1)  # Number of values to drop (10% of total)\n",
        "    \n",
        "    if num_values_to_drop == 0:\n",
        "        return tensor\n",
        "    \n",
        "    flattened_tensor = tensor.flatten()  # Flatten the tensor into a 1D tensor\n",
        "    sorted_indices = torch.argsort(flattened_tensor)  # Sort the indices based on the tensor values\n",
        "    \n",
        "    # Set the lowest values to 0\n",
        "    sorted_indices_to_drop = sorted_indices[:num_values_to_drop]\n",
        "    flattened_tensor[sorted_indices_to_drop] = 0\n",
        "    \n",
        "    # Reshape the tensor back to its original shape\n",
        "    dropped_tensor = flattened_tensor.reshape(tensor.shape)\n",
        "    \n",
        "    return dropped_tensor"
      ],
      "metadata": {
        "id": "tIGZKyAmETEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GumbleSoftmax(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, input):\n",
        "        gumbel_noise = torch.rand_like(input)\n",
        "        gumbel_noise = -torch.log(-torch.log(gumbel_noise + 1e-20) + 1e-20)\n",
        "        logits = (input + gumbel_noise) / self.temperature\n",
        "        return F.softmax(logits, dim=1)"
      ],
      "metadata": {
        "id": "c49cI7uLXfta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogGumbleSoftmax(nn.Module):\n",
        "    def __init__(self, temperature):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, input):\n",
        "        gumbel_noise = torch.rand_like(input)\n",
        "        gumbel_noise = -torch.log(-torch.log(gumbel_noise + 1e-20) + 1e-20)\n",
        "        logits = (input + gumbel_noise) / self.temperature\n",
        "        return F.log_softmax(logits, dim=1)"
      ],
      "metadata": {
        "id": "knJJxxkgsjFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PseudoSoftmax(nn.Module):\n",
        "    def forward(self, input):\n",
        "        exp_input = torch.pow(2, input)\n",
        "        softmax_output = exp_input / torch.sum(exp_input, dim=1, keepdim=True)\n",
        "        return softmax_output"
      ],
      "metadata": {
        "id": "KIP1nUUuYDK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "winp9QsNmiSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YMRcG6zdtU2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Resnet12 Layer Structure\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
        "              nn.BatchNorm2d(out_channels), \n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class ResNet12(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        \n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        \n",
        "        self.conv5 = conv_block(512, 1024, pool=True)\n",
        "        self.res3 = nn.Sequential(conv_block(1024, 1024), conv_block(1024, 1024))\n",
        "        \n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(2), \n",
        "                                        nn.Flatten(), \n",
        "                                        nn.Linear(1024, num_classes),\n",
        "                                        # GumbleSoftmax(temperature = 3),\n",
        "                                        PseudoSoftmax(),\n",
        "                                        )\n",
        "        \n",
        "        # self.softmax_layer = nn.LogSoftmax(dim = 1)\n",
        "        # self.softmax_layer = nn.Softmax(dim = 1)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.conv5(out)\n",
        "        out = self.res3(out) + out\n",
        "        out = self.classifier(out)\n",
        "\n",
        "        \n",
        "        # out = drop_lowest_values(out)\n",
        "\n",
        "        # out = self.softmax_layer(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "BavB8tllrEF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        " # for evaluation thingie"
      ],
      "metadata": {
        "id": "q1MLjOzNJEsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    \n",
        "    # Set up custom optimizer with weight decay\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    # Set up one-cycle learning rate scheduler\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
        "                                                steps_per_epoch=len(train_loader))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Gradient clipping\n",
        "            if grad_clip: \n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Record & update learning rate\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        \n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    \n",
        "     # After completing training, evaluate on the validation set\n",
        "    model.eval()\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "    for batch in val_loader:\n",
        "        outputs = model(batch[0])\n",
        "        predicted_labels.extend(torch.argmax(outputs, dim=1).tolist())\n",
        "        true_labels.extend(batch[1].tolist())\n",
        "\n",
        "    precision = precision_score(true_labels, predicted_labels, average='macro')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='macro')\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "    history[-1]['precision'] = precision\n",
        "    history[-1]['recall'] = recall\n",
        "    history[-1]['f1'] = f1\n",
        "    history[-1]['confusion_matrix'] = cm\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "bXwpdpFzmn--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1rDznn6rayE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet12 = to_device(ResNet12(3, 100), device)\n",
        "model_resnet12"
      ],
      "metadata": {
        "id": "TfXpg6xsrSgC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f19042b-a567-4271-91a9-f59424b34fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet12(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res1): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv4): Sequential(\n",
              "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res2): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (conv5): Sequential(\n",
              "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res3): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Linear(in_features=1024, out_features=100, bias=True)\n",
              "    (3): PseudoSoftmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model_resnet12, (3,32,32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMtbDUl_s2zq",
        "outputId": "c95e6309-a840-4ba5-b371-d73b73e67712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4          [-1, 128, 32, 32]          73,856\n",
            "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
            "              ReLU-6          [-1, 128, 32, 32]               0\n",
            "         MaxPool2d-7          [-1, 128, 16, 16]               0\n",
            "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
            "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
            "             ReLU-10          [-1, 128, 16, 16]               0\n",
            "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
            "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
            "             ReLU-13          [-1, 128, 16, 16]               0\n",
            "           Conv2d-14          [-1, 256, 16, 16]         295,168\n",
            "      BatchNorm2d-15          [-1, 256, 16, 16]             512\n",
            "             ReLU-16          [-1, 256, 16, 16]               0\n",
            "        MaxPool2d-17            [-1, 256, 8, 8]               0\n",
            "           Conv2d-18            [-1, 512, 8, 8]       1,180,160\n",
            "      BatchNorm2d-19            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-20            [-1, 512, 8, 8]               0\n",
            "        MaxPool2d-21            [-1, 512, 4, 4]               0\n",
            "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-24            [-1, 512, 4, 4]               0\n",
            "           Conv2d-25            [-1, 512, 4, 4]       2,359,808\n",
            "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-27            [-1, 512, 4, 4]               0\n",
            "           Conv2d-28           [-1, 1024, 4, 4]       4,719,616\n",
            "      BatchNorm2d-29           [-1, 1024, 4, 4]           2,048\n",
            "             ReLU-30           [-1, 1024, 4, 4]               0\n",
            "        MaxPool2d-31           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-32           [-1, 1024, 2, 2]       9,438,208\n",
            "      BatchNorm2d-33           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-34           [-1, 1024, 2, 2]               0\n",
            "           Conv2d-35           [-1, 1024, 2, 2]       9,438,208\n",
            "      BatchNorm2d-36           [-1, 1024, 2, 2]           2,048\n",
            "             ReLU-37           [-1, 1024, 2, 2]               0\n",
            "        MaxPool2d-38           [-1, 1024, 1, 1]               0\n",
            "          Flatten-39                 [-1, 1024]               0\n",
            "           Linear-40                  [-1, 100]         102,500\n",
            "    PseudoSoftmax-41                  [-1, 100]               0\n",
            "================================================================\n",
            "Total params: 30,274,916\n",
            "Trainable params: 30,274,916\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 9.67\n",
            "Params size (MB): 115.49\n",
            "Estimated Total Size (MB): 125.17\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_resnet12 = [evaluate(model_resnet12, val_loader)]\n",
        "history_resnet12"
      ],
      "metadata": {
        "id": "sjPN5_3brUFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ccc681c-e746-4d95-f696-20dc87c489fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYeBtWaBmolO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9NC0nTPmqx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting parameters for training**"
      ],
      "metadata": {
        "id": "Q8lYrWoGms1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_resnet12 = 50\n",
        "max_lr = 0.01\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-5\n",
        "opt_func = torch.optim.Adam"
      ],
      "metadata": {
        "id": "J3jqtMb9rfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history_resnet12 += fit_one_cycle(epochs_resnet12, max_lr, model_resnet12, train_loader, val_loader, \n",
        "                             grad_clip=grad_clip, \n",
        "                             weight_decay=weight_decay, \n",
        "                             opt_func=opt_func)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Dj166LrgJO",
        "outputId": "bef84e7a-67ea-447b-af20-2dcf37d9ebc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], last_lr: 0.00050, train_loss: 4.5764, val_loss: 4.5716, val_acc: 0.0492\n",
            "Epoch [1], last_lr: 0.00081, train_loss: 4.5587, val_loss: 4.5533, val_acc: 0.0690\n",
            "Epoch [2], last_lr: 0.00132, train_loss: 4.5525, val_loss: 4.5551, val_acc: 0.0668\n",
            "Epoch [3], last_lr: 0.00199, train_loss: 4.5492, val_loss: 4.5652, val_acc: 0.0570\n",
            "Epoch [4], last_lr: 0.00280, train_loss: 4.5479, val_loss: 4.5467, val_acc: 0.0758\n",
            "Epoch [5], last_lr: 0.00371, train_loss: 4.5457, val_loss: 4.5458, val_acc: 0.0768\n",
            "Epoch [6], last_lr: 0.00470, train_loss: 4.5526, val_loss: 4.5540, val_acc: 0.0688\n",
            "Epoch [7], last_lr: 0.00570, train_loss: 4.5604, val_loss: 4.5701, val_acc: 0.0517\n",
            "Epoch [8], last_lr: 0.00668, train_loss: 4.5691, val_loss: 4.5772, val_acc: 0.0453\n",
            "Epoch [9], last_lr: 0.00760, train_loss: 4.5672, val_loss: 4.5878, val_acc: 0.0336\n",
            "Epoch [10], last_lr: 0.00841, train_loss: 4.5655, val_loss: 4.5788, val_acc: 0.0427\n",
            "Epoch [11], last_lr: 0.00908, train_loss: 4.5633, val_loss: 4.5771, val_acc: 0.0444\n",
            "Epoch [12], last_lr: 0.00958, train_loss: 4.5625, val_loss: 4.5686, val_acc: 0.0521\n",
            "Epoch [13], last_lr: 0.00990, train_loss: 4.5596, val_loss: 4.5701, val_acc: 0.0515\n",
            "Epoch [14], last_lr: 0.01000, train_loss: 4.5556, val_loss: 4.5732, val_acc: 0.0478\n",
            "Epoch [15], last_lr: 0.00998, train_loss: 4.5545, val_loss: 4.5687, val_acc: 0.0528\n",
            "Epoch [16], last_lr: 0.00992, train_loss: 4.5518, val_loss: 4.5546, val_acc: 0.0674\n",
            "Epoch [17], last_lr: 0.00982, train_loss: 4.5421, val_loss: 4.5418, val_acc: 0.0801\n",
            "Epoch [18], last_lr: 0.00968, train_loss: 4.5324, val_loss: 4.5389, val_acc: 0.0827\n",
            "Epoch [19], last_lr: 0.00950, train_loss: 4.5217, val_loss: 4.5309, val_acc: 0.0918\n",
            "Epoch [20], last_lr: 0.00929, train_loss: 4.5140, val_loss: 4.5388, val_acc: 0.0830\n",
            "Epoch [21], last_lr: 0.00905, train_loss: 4.5119, val_loss: 4.5229, val_acc: 0.0985\n",
            "Epoch [22], last_lr: 0.00877, train_loss: 4.5081, val_loss: 4.5227, val_acc: 0.0998\n",
            "Epoch [23], last_lr: 0.00846, train_loss: 4.4999, val_loss: 4.5107, val_acc: 0.1110\n",
            "Epoch [24], last_lr: 0.00812, train_loss: 4.4953, val_loss: 4.5043, val_acc: 0.1174\n",
            "Epoch [25], last_lr: 0.00775, train_loss: 4.4920, val_loss: 4.5110, val_acc: 0.1119\n",
            "Epoch [26], last_lr: 0.00737, train_loss: 4.4876, val_loss: 4.5005, val_acc: 0.1224\n",
            "Epoch [27], last_lr: 0.00697, train_loss: 4.4841, val_loss: 4.4964, val_acc: 0.1267\n",
            "Epoch [28], last_lr: 0.00655, train_loss: 4.4804, val_loss: 4.4988, val_acc: 0.1231\n",
            "Epoch [29], last_lr: 0.00611, train_loss: 4.4768, val_loss: 4.5010, val_acc: 0.1213\n",
            "Epoch [30], last_lr: 0.00567, train_loss: 4.4726, val_loss: 4.4917, val_acc: 0.1300\n",
            "Epoch [31], last_lr: 0.00522, train_loss: 4.4679, val_loss: 4.4887, val_acc: 0.1344\n",
            "Epoch [32], last_lr: 0.00478, train_loss: 4.4646, val_loss: 4.4899, val_acc: 0.1334\n",
            "Epoch [33], last_lr: 0.00433, train_loss: 4.4614, val_loss: 4.4853, val_acc: 0.1375\n",
            "Epoch [34], last_lr: 0.00389, train_loss: 4.4564, val_loss: 4.4853, val_acc: 0.1379\n",
            "Epoch [35], last_lr: 0.00345, train_loss: 4.4520, val_loss: 4.4875, val_acc: 0.1353\n",
            "Epoch [36], last_lr: 0.00303, train_loss: 4.4489, val_loss: 4.4860, val_acc: 0.1374\n",
            "Epoch [37], last_lr: 0.00263, train_loss: 4.4446, val_loss: 4.4825, val_acc: 0.1406\n",
            "Epoch [38], last_lr: 0.00225, train_loss: 4.4415, val_loss: 4.4833, val_acc: 0.1394\n",
            "Epoch [39], last_lr: 0.00188, train_loss: 4.4386, val_loss: 4.4808, val_acc: 0.1423\n",
            "Epoch [40], last_lr: 0.00154, train_loss: 4.4365, val_loss: 4.4798, val_acc: 0.1432\n",
            "Epoch [41], last_lr: 0.00123, train_loss: 4.4335, val_loss: 4.4806, val_acc: 0.1435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxczLphrrh5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xpZTe8-lmsJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OLNZzG4Nmwth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "P8tvebVsmz8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(50, 50))\n",
        "    # plt.imshow(cm, interpolation='nearest', cmap=plt.cm.OrRd)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.colormaps['magma'])\n",
        "\n",
        "    ## https://matplotlib.org/stable/tutorials/colors/colormaps.html#lightness-of-matplotlib-colormaps\n",
        "\n",
        "    plt.colorbar(shrink=0.50)\n",
        "    \n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "last_result = history_resnet12[-1]\n",
        "cm = last_result['confusion_matrix']\n",
        "plot_confusion_matrix(cm, classes)"
      ],
      "metadata": {
        "id": "abTUJkvHJRCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_result = history_resnet12[-1]\n",
        "precision = last_result['precision']\n",
        "recall = last_result['recall']\n",
        "f1 = last_result['f1']\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "9f2VlVBbOFBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-o', color = 'black')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs. Number of Epochs');"
      ],
      "metadata": {
        "id": "7XmZPAM8nAED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_accuracies(history_resnet12)"
      ],
      "metadata": {
        "id": "Hj4rtYz_m1lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(history):\n",
        "    train_losses = [x.get('train_loss') for x in history]\n",
        "    val_losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(train_losses, color = 'green')\n",
        "    plt.plot(val_losses, color = 'brown')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "    plt.title('Loss vs. Number of epochs');"
      ],
      "metadata": {
        "id": "tQFF_RpQm25J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(history_resnet12)"
      ],
      "metadata": {
        "id": "BOCYarB-m4Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.plot(lrs, color = 'orange')\n",
        "    plt.xlabel('Batch Number')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.title('Learning Rate vs. Batch Number');"
      ],
      "metadata": {
        "id": "JgggncVsm5L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_lrs(history_resnet12)"
      ],
      "metadata": {
        "id": "K-bbz2d1m6LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_resnet12 = evaluate(model_resnet12, test_loader) #final evaluation of resnet9 model with test dataset\n",
        "test_resnet12"
      ],
      "metadata": {
        "id": "kx2aJmt9nFy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_resnet12 = test_resnet12['val_loss']\n",
        "test_acc_resnet12 = test_resnet12['val_acc']\n",
        "print('test_loss_resnet9:', test_resnet12['val_loss'])\n",
        "print('test_acc_resnet9:', test_resnet12['val_acc'])"
      ],
      "metadata": {
        "id": "aSW5bNs6nHBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_resnet12.state_dict(), 'ResNet12_v1_gumbel_softmax.pth')"
      ],
      "metadata": {
        "id": "hg8cFksonIqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(img, model_resnet12):\n",
        "    # Convert to a batch of 1\n",
        "    xb = to_device(img.unsqueeze(0), device)\n",
        "    # Get predictions from model\n",
        "    yb = model_resnet9(xb)\n",
        "    # Pick index with highest probability\n",
        "    _, preds  = torch.max(yb, dim=1)\n",
        "\n",
        "    print(yb)\n",
        "    # Retrieve the class label\n",
        "    # return yb\n",
        "\n",
        "    return dataset.classes[preds[0].item()]"
      ],
      "metadata": {
        "id": "kwx-QAPwnMxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# def predict_image(img, model_resnet9):\n",
        "#     # Convert to a batch of 1\n",
        "#     xb = to_device(img.unsqueeze(0), device)\n",
        "#     # Get predictions from model\n",
        "#     yb = model_resnet9(xb)\n",
        "#     # Apply softmax to convert to probabilities\n",
        "#     probs = F.log_softmax(yb, dim=1)\n",
        "#     # Pick index with highest probability\n",
        "#     _, preds = torch.max(probs, dim=1)\n",
        "#     # Retrieve the class label\n",
        "#     print(probs)\n",
        "#     print(preds.shape)\n",
        "#     # return probs\n",
        "#     predicted_label = dataset.classes[preds[0].item()]\n",
        "#     return predicted_label\n"
      ],
      "metadata": {
        "id": "Rji2mmxlCvD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M3AMcZ64DcgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, label = test_dataset[3480]\n",
        "plt.imshow(img.permute(1, 2, 0))\n",
        "print('Label:', dataset.classes[label], ', Predicted:', predict_image(img, model_resnet12))"
      ],
      "metadata": {
        "id": "jzTdBCapnNwk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}